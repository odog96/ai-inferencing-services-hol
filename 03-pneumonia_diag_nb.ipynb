{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1031639-fc7b-48fe-abf7-c526b0beff28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import httpx\n",
    "from PIL import Image # Import Pillow for image processing\n",
    "import numpy as np # Import numpy for array manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "703fd37d-393e-403d-a47f-7f854e77bb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is ready.\n",
      "\n",
      "Model Metadata:\n",
      "{\n",
      "  \"name\": \"rtcn-dmnq-p6oh-4e8f\",\n",
      "  \"versions\": [\n",
      "    \"1\"\n",
      "  ],\n",
      "  \"platform\": \"onnxruntime_onnx\",\n",
      "  \"inputs\": [\n",
      "    {\n",
      "      \"name\": \"input\",\n",
      "      \"datatype\": \"FP32\",\n",
      "      \"shape\": [\n",
      "        -1,\n",
      "        1,\n",
      "        224,\n",
      "        224\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"name\": \"output\",\n",
      "      \"datatype\": \"FP32\",\n",
      "      \"shape\": [\n",
      "        -1,\n",
      "        2\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Detected Input Name: input\n",
      "Detected Input Shape: [-1, 1, 224, 224]\n",
      "Detected Input Datatype: FP32\n",
      "Target image size for model: (224, 224)\n",
      "Preprocessed image array shape: (1, 1, 224, 224)\n",
      "Image data flattened. Total elements: 50176\n",
      "\n",
      "Making inference request...\n",
      "\n",
      "Inference Response:\n",
      "{\n",
      "  \"model_name\": \"rtcn-dmnq-p6oh-4e8f\",\n",
      "  \"model_version\": \"1\",\n",
      "  \"outputs\": [\n",
      "    {\n",
      "      \"name\": \"output\",\n",
      "      \"shape\": [\n",
      "        1,\n",
      "        2\n",
      "      ],\n",
      "      \"datatype\": \"FP32\",\n",
      "      \"data\": [\n",
      "        -8.839442253112793,\n",
      "        10.951461791992188\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have these installed:\n",
    "# pip install open-inference-openapi httpx Pillow numpy\n",
    "\n",
    "from open_inference.openapi.client import OpenInferenceClient, InferenceRequest\n",
    "\n",
    "#If running from workbench use /tmp/jwt. Otherwise provide your CDP_TOKEN\n",
    "#Make sure to replace \"/tmp/jwt\" with the actual path to your JWT file if different\n",
    "try:\n",
    "    API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: /tmp/jwt not found. Please ensure you have a valid CDP_TOKEN or JWT file.\")\n",
    "    # You might want to prompt the user for API_KEY here or exit\n",
    "\n",
    "\n",
    "BASE_URL = ''\n",
    "MODEL_NAME = ''\n",
    "headers = {\n",
    "\t'Authorization': 'Bearer ' + API_KEY,\n",
    "\t'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "httpx_client = httpx.Client(headers=headers)\n",
    "client = OpenInferenceClient(base_url=BASE_URL, httpx_client=httpx_client)\n",
    "\n",
    "# Check that the server is live, and it has the model loaded\n",
    "try:\n",
    "    client.check_server_readiness()\n",
    "    print(\"Server is ready.\")\n",
    "    metadata = client.read_model_metadata(MODEL_NAME)\n",
    "    metadata_str = json.dumps(json.loads(metadata.json()), indent=2)\n",
    "    print(\"\\nModel Metadata:\")\n",
    "    print(metadata_str)\n",
    "\n",
    "    # --- Extract input details from metadata ---\n",
    "    # Assuming the model has at least one input and it's an image\n",
    "    model_inputs_meta = json.loads(metadata.json()).get(\"inputs\", [])\n",
    "    if not model_inputs_meta:\n",
    "        raise ValueError(\"Model metadata does not contain input information.\")\n",
    "\n",
    "    # For simplicity, we'll assume the first input in the metadata is the one we need.\n",
    "    # In a real application, you might need to iterate or check names if there are multiple inputs.\n",
    "    input_name = model_inputs_meta[0][\"name\"]\n",
    "    input_shape = model_inputs_meta[0][\"shape\"]\n",
    "    input_datatype = model_inputs_meta[0][\"datatype\"]\n",
    "\n",
    "    print(f\"\\nDetected Input Name: {input_name}\")\n",
    "    print(f\"Detected Input Shape: {input_shape}\")\n",
    "    print(f\"Detected Input Datatype: {input_datatype}\")\n",
    "\n",
    "    # --- Image Loading and Preprocessing ---\n",
    "    # Define the path to your local JPEG image\n",
    "    # IMPORTANT: Replace 'path/to/your/image.jpeg' with the actual path to your image file\n",
    "    IMAGE_PATH = \"/home/cdsw/person1946_bacteria_4874.jpeg\" # e.g., 'patient_xray.jpeg'\n",
    "\n",
    "    # Determine the target image size from the model's input shape\n",
    "    # The model expects [-1, 1, 224, 224] -> [BATCH, CHANNEL, HEIGHT, WIDTH]\n",
    "    # Pillow's resize expects (width, height)\n",
    "    if len(input_shape) >= 3: # Ensure there are enough dimensions for height and width\n",
    "        target_height = input_shape[-1] # Last dimension is width\n",
    "        target_width = input_shape[-2] # Second to last dimension is height\n",
    "        target_size = (target_height, target_width) # Pillow expects (width, height)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected input shape for image: {input_shape}. Cannot determine resize dimensions.\")\n",
    "\n",
    "    print(f\"Target image size for model: {target_size}\")\n",
    "\n",
    "    try:\n",
    "        img = Image.open(IMAGE_PATH)\n",
    "        img = img.resize(target_size) # Resize image to model's expected dimensions (224x224)\n",
    "        \n",
    "        # Convert to grayscale ('L' for Luminance) as the model expects 1 channel\n",
    "        if img.mode != 'L':\n",
    "            img = img.convert('L')\n",
    "\n",
    "        # Convert image to a numpy array\n",
    "        img_array = np.asarray(img) # Shape will be (HEIGHT, WIDTH) for grayscale\n",
    "\n",
    "        # Normalize pixel values if the model expects FP32 in a certain range (e.g., 0-1)\n",
    "        # This is a common step for deep learning models. Adjust as per your model's training.\n",
    "        if input_datatype == \"FP32\":\n",
    "            img_array = img_array / 255.0 # Normalize to 0-1 range for FP32\n",
    "\n",
    "        # Add batch dimension and channel dimension, then transpose to [BATCH, CHANNEL, HEIGHT, WIDTH]\n",
    "        # Current img_array shape for grayscale is (HEIGHT, WIDTH)\n",
    "        # Add channel dimension (1) at axis 0: (1, HEIGHT, WIDTH)\n",
    "        # Add batch dimension (1) at axis 0: (1, 1, HEIGHT, WIDTH)\n",
    "        img_array = img_array[np.newaxis, np.newaxis, :, :]\n",
    "\n",
    "        # Verify the final shape before flattening\n",
    "        print(f\"Preprocessed image array shape: {img_array.shape}\")\n",
    "\n",
    "        # Flatten the numpy array to a 1D list\n",
    "        image_data_flat = img_array.flatten().tolist()\n",
    "        print(f\"Image data flattened. Total elements: {len(image_data_flat)}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {IMAGE_PATH}. Please check the path.\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Make an inference request\n",
    "    print(\"\\nMaking inference request...\")\n",
    "    pred = client.model_infer(\n",
    "        MODEL_NAME,\n",
    "        request=InferenceRequest(\n",
    "            inputs=[\n",
    "                {\n",
    "                    \"name\": input_name,\n",
    "                    \"shape\": list(img_array.shape), # Use the actual shape of the prepared numpy array (e.g., [1, 1, 224, 224])\n",
    "                    \"datatype\": input_datatype,\n",
    "                    \"data\": image_data_flat\n",
    "                }\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    json_resp_str = json.dumps(json.loads(pred.json()), indent=2)\n",
    "    print(\"\\nInference Response:\")\n",
    "    print(json_resp_str)\n",
    "\n",
    "except httpx.HTTPStatusError as e:\n",
    "    print(f\"HTTP Error: {e.response.status_code} - {e.response.text}\")\n",
    "    print(\"Please check your BASE_URL, MODEL_NAME, and API_KEY.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f910ac27-9865-4a54-bda6-7f27dd97d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to an index\n",
    "output = json.loads(json_resp_str)[\"outputs\"][0]['data']\n",
    "\n",
    "# Convert logits to numpy array for softmax\n",
    "logits_np = np.array(output)\n",
    "\n",
    "# Apply softmax function for numerical stability\n",
    "exp_logits = np.exp(logits_np - np.max(logits_np))\n",
    "probabilities = np.round(exp_logits / np.sum(exp_logits),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a37cfce-663e-40b6-8ab4-51f8a562df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosis - Pneumonia : Probability 1.0\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(probabilities)\n",
    "max_value = np.max(probabilities)\n",
    "\n",
    "if max_index == 1:\n",
    "    print(f'Diagnosis - Pneumonia : Probability {max_value}')\n",
    "else:\n",
    "    print(f'Diagnosis - No Pneumonia : Probability {max_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04695450-253d-4f88-b5d8-4268aafe1bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
