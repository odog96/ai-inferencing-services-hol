{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d257d154-5507-4f61-ab1f-a05e87eaf435",
   "metadata": {},
   "source": [
    "# Model API Integration with Cloudera AI\n",
    "\n",
    "This notebook demonstrates the flexibility of Cloudera AI inferencing services by showing different ways to interact with deployed models. We'll progress from basic API usage to more complex implementations, showing how easy it is to switch between different models and frameworks.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10 or later\n",
    "- Access to Cloudera AI console\n",
    "- Two deployed models: test-model-llama-8b-v2 and deepseek-r1-distill-llama-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d30e1a-fad2-4a45-971f-329d133a4f19",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Before proceeding, you'll need to gather information from your deployed models in the Cloudera AI console:\n",
    "\n",
    "1. Go to Cloudera AI console > Model Endpoints\n",
    "2. Find the models: \n",
    "   - llama 33 70b\n",
    "   - Mistral model\n",
    "3. For each model:\n",
    "   - Copy the endpoint URL (remove everything after /v1) for example :\n",
    "   - `https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/...../modelxyz/openai/v1/chat/completions`\n",
    "   - would be converted to :\n",
    "   - `https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/...../modelxyz/openai/v1`\n",
    "   - Copy the Model ID\n",
    "\n",
    "The first model's information will go into `base_url` and `model_name` variables. The 2nd model will be `ds_base_url` and `ds_model_name` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "715f17e3-fef9-461e-b4a2-d0af5631b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "import asyncio\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa72c8-1ff2-4146-8382-2e04d913ec11",
   "metadata": {},
   "source": [
    "#### Model enpoint collection\n",
    "1. Collect Llama 3.1 model endpont details\n",
    "2. Collect Deepseek model enpoint details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29991406-dd0e-4da8-8c74-03ad956cc830",
   "metadata": {},
   "source": [
    "go to cloudera AI and get the following parameters.Cut off tail end of url after '/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e6f82-538a-4f32-ac2d-a2f38d7ba169",
   "metadata": {},
   "source": [
    "**Llama 3.3 70b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77af687a-1b4e-4be9-929f-dbd74ba23e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://ml-c4fba600-96e.eng-ml-l.vnu8-sqze.cloudera.site/namespaces/serving-default/endpoints/llama-33-70b/v1\"\n",
    "model_name = \"meta/llama-3.3-70b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ddd00-b0bd-46c9-9f14-a72248b01183",
   "metadata": {},
   "source": [
    "**2nd model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6ea7aca-d396-4a54-af6b-1bc093d97778",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_base_url = \"https://ml-c4fba600-96e.eng-ml-l.vnu8-sqze.cloudera.site/namespaces/serving-default/endpoints/llama-test-p8s/v1\"\n",
    "ds_model_name = \"meta/llama-3.2-11b-vision-instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcb22c-5562-4976-b41a-7bb31af5f97a",
   "metadata": {},
   "source": [
    "#### Auth setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78495-5b5a-4ee1-8b33-2d8f1ea766e7",
   "metadata": {},
   "source": [
    "Here is the auth token you'll use to connect to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e226feb-d60f-436d-ae9c-aaf460df8c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJqa3UiOiJodHRwczpcL1wvZW5nLW1sLWxyLXByb2QtZW52LXYyLWF3cy1kbC1nYXRld2F5MS5lbmctbWwtbC52bnU4LXNxemUuY2xvdWRlcmEuc2l0ZTo4NDQzXC9lbmctbWwtbHItcHJvZC1lbnYtdjItYXdzLWRsXC9rdC1rZXJiZXJvc1wva25veHRva2VuXC9hcGlcL3YxXC9qd2tzLmpzb24iLCJraWQiOiJBejBzMG0tUjlJcHlIc0NCdlZhR1ZKX2hWVGltNERzeGhQRVRXTVdaelQ0IiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJvemFyYXRlIiwiamt1IjoiaHR0cHM6XC9cL2VuZy1tbC1sci1wcm9kLWVudi12Mi1hd3MtZGwtZ2F0ZXdheTEuZW5nLW1sLWwudm51OC1zcXplLmNsb3VkZXJhLnNpdGU6ODQ0M1wvZW5nLW1sLWxyLXByb2QtZW52LXYyLWF3cy1kbFwva3Qta2VyYmVyb3NcL2tub3h0b2tlblwvYXBpXC92MVwvandrcy5qc29uIiwia2lkIjoiQXowczBtLVI5SXB5SHNDQnZWYUdWSl9oVlRpbTREc3hoUEVUV01XWnpUNCIsImlzcyI6IktOT1hTU08iLCJleHAiOjE3NTM3NjMyMTQsIm1hbmFnZWQudG9rZW4iOiJmYWxzZSIsImtub3guaWQiOiI5ZTcwYzc3ZS1iOTExLTQxYTEtOGUzYS04YjU4NDQyMmRlNzMifQ.aQUgIVl77-5jsBx0Xu4Skkd16SwEfhrFj2IlLEWYLI3Mo_DgT6JD1m72HEGHD1w7vhI8sUWww4GIV8XiuiIzfwpKQeuvEALJOcD0TFag2yfRMG-eKRu9yNz9JFWUoq8WQXvYvLWU7tDvaRSVus3HooVKljMUXCbtrvex0awJ6Qvvcb6A5LgUyF3k3msn2SjuTGq9bfI7peLDMjFfXIWe7fzw9mKL34yOiIE3pjWnQRLERrBZk4jgSKcFtrN3nr2JGt6IHTLm6W4Ck_i4ydjIrAhaRCNK8wGt8pnBPb6S2lPx6KROjI9HuI15bFFFvKEkHYRrgwdw1sGumcri-5HXWA'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90547c3c-2ec0-433a-bd62-31b7d62862ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef92fa88-473e-4bf0-9475-9df2cab9293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "\tbase_url=base_url,\n",
    "\tapi_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce0197-ec1e-4693-82fb-183a84b78a98",
   "metadata": {},
   "source": [
    "## Basic Model Interaction\n",
    "\n",
    "This section demonstrates the simplest way to interact with our deployed model through the OpenAI package. We'll:\n",
    "1. Create a client with our model's endpoint and authentication\n",
    "2. Send a simple message to test the connection\n",
    "3. Display the model's streaming response\n",
    "\n",
    "This represents the most straightforward way to interact with the model, similar to how you might use OpenAI's API. The key difference is that we're using our own deployed model through Cloudera AI's infrastructure.\n",
    "\n",
    "Note: We're using streaming=True in our completion request, which means we'll see the response being generated token by token, providing a more interactive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9d6836-5df3-4d7b-9f8c-0effe670015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Write a one-sentence definition of GenAI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4d28a9a-9362-4459-af29-f52d826b27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, or General Artificial Intelligence, refers to a type of artificial intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence, enabling it to perform any intellectual task that a human can."
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[{\"role\":\"user\",\"content\":message}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c6688-b736-42f4-8b87-b569b656b78e",
   "metadata": {},
   "source": [
    "## Enhanced Chat Client Implementation\n",
    "\n",
    "This section implements a stateful chat client that maintains conversation history and can handle streaming responses. It demonstrates how to build more complex applications while maintaining the simple interface of the basic client.\n",
    "\n",
    "Key features:\n",
    "- Conversation history tracking\n",
    "- Streaming response support\n",
    "- Configurable parameters\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "aa52d379-0f54-4c23-b6d5-091c1b31116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatClient:\n",
    "    def __init__(self, model_name: str, base_url: str):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Set up HTTP client\n",
    "        if \"CUSTOM_CA_STORE\" not in os.environ:\n",
    "            http_client = httpx.Client()\n",
    "        else:\n",
    "            http_client = httpx.Client(verify=os.environ[\"CUSTOM_CA_STORE\"])\n",
    "            \n",
    "        # Load API key\n",
    "        OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            http_client=http_client,\n",
    "        )\n",
    "        \n",
    "        self.conversation_history: List[Dict[str, str]] = []\n",
    "\n",
    "    def chat(self, message: str, stream: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Send a message to the chat model and get the response.\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                partial_message = \"\"\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=self.conversation_history + [{\"role\": \"system\", \"content\": \"After your thinking, always provide a clear, structured answer.\"}],\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.7,\n",
    "                    max_tokens=1024,  # Further increased token limit\n",
    "                    stream=True,\n",
    "                )\n",
    "                \n",
    "                for chunk in response:\n",
    "                    if chunk.choices[0].delta.content is not None:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        partial_message += content\n",
    "\n",
    "                \n",
    "                final_message = partial_message\n",
    "\n",
    "            else:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=self.conversation_history,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.7,\n",
    "                    max_tokens=512,  # Increased token limit\n",
    "                    stream=False,\n",
    "                )\n",
    "                \n",
    "                complete_response = response.choices[0].message.content\n",
    "                #print(\"\\nNon-streaming response:\", repr(complete_response))\n",
    "                \n",
    "                final_message = complete_response\n",
    "                # if self.deepseek_clean:\n",
    "                #     final_message = self._clean_response(complete_response)\n",
    "                #     #print(\"\\nAfter cleaning:\", repr(final_message))\n",
    "                #     print(repr(final_message))\n",
    "            # Only add to history if we got a valid response\n",
    "            if final_message:\n",
    "                self.conversation_history.append({\"role\": \"assistant\", \"content\": final_message})\n",
    "            \n",
    "            return final_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in chat method: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the conversation history.\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d6603b52-e28c-4ea1-ac41-d41f11a9f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "chat_client = ChatClient(model_name, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8e7891e-27e8-4781-87cf-91a47868cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"in 6 sentences or less explain how weights get update during model training process of a neural network. Explain this to a 6 year old'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e6b9e82-520a-4b71-8086-f4d75f7f87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8d87608-c072-4720-acce-f80407e20e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "message2 = \"now do the same thing but explain learning rate. 5 sentences or less\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cd929-ff06-44f0-8ec6-072b62180489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message2, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0375d9-5682-4208-9531-e78227fa1c02",
   "metadata": {},
   "source": [
    "## Model Switching Demonstration\n",
    "\n",
    "One of the key benefits of Cloudera AI is the ability to easily switch between different models. Here we'll demonstrate this by changing to the second model while using the same code structure.\n",
    "\n",
    "For this section, we'll use our second model's information:\n",
    "- URL goes into `ds_base_url` (remember to clip after /v1)\n",
    "- Model ID goes into `ds_model_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116c314-bdb2-47e5-bbda-a863a35130a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "new_chat_client = ChatClient(ds_model_name,ds_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aed5cd-e100-4fed-a563-8210894eb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message3 = \"in 5 sentences or less what is a learning rate in neural networks?\"\n",
    "response_ds = new_chat_client.chat(message3, stream=True)\n",
    "#print(response_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
