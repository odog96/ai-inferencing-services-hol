{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d257d154-5507-4f61-ab1f-a05e87eaf435",
   "metadata": {},
   "source": [
    "# Model API Integration with Cloudera AI\n",
    "\n",
    "This notebook demonstrates the flexibility of Cloudera AI inferencing services by showing different ways to interact with deployed models. We'll progress from basic API usage to more complex implementations, showing how easy it is to switch between different models and frameworks.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10 or later\n",
    "- Access to Cloudera AI console\n",
    "- Two deployed models: test-model-llama-8b-v2 and deepseek-r1-distill-llama-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d30e1a-fad2-4a45-971f-329d133a4f19",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Before proceeding, you'll need to gather information from your deployed models in the Cloudera AI console:\n",
    "\n",
    "1. Go to Cloudera AI console > Model Endpoints\n",
    "2. Find the models: \n",
    "   - test-model-llama-8b-v2\n",
    "   - deepseek-r1-distill-llama-8b\n",
    "3. For each model:\n",
    "   - Copy the endpoint URL (remove everything after /v1) for example :\n",
    "   - `https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/...../modelxyz/openai/v1/chat/completions`\n",
    "   - would be converted to :\n",
    "   - `https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/...../modelxyz/openai/v1`\n",
    "   - Copy the Model ID\n",
    "\n",
    "The first model's information will go into `base_url` and `model_name` variables. The 2nd model will be `ds_base_url` and `ds_model_name` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "715f17e3-fef9-461e-b4a2-d0af5631b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "# For Lang chain:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import asyncio\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa72c8-1ff2-4146-8382-2e04d913ec11",
   "metadata": {},
   "source": [
    "#### Model enpoint collection\n",
    "1. Collect Llama 3.1 model endpont details\n",
    "2. Collect Deepseek model enpoint details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29991406-dd0e-4da8-8c74-03ad956cc830",
   "metadata": {},
   "source": [
    "go to cloudera AI and get the following parameters.Cut off tail end of url after '/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e6f82-538a-4f32-ac2d-a2f38d7ba169",
   "metadata": {},
   "source": [
    "**Llama 3.1 8b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af687a-1b4e-4be9-929f-dbd74ba23e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"enter-url here.\"\n",
    "#model_name = \"enter model name here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa6acc2-9ae3-4b0f-b4d6-7b19d8d6659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"enter-url here.\"\n",
    "base_url = \"https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/namespaces/serving-default/endpoints/test-model-llama-8b-v2/v1\"\n",
    "#model_name = \"enter model name here\"\n",
    "model_name = \"meta/llama-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ddd00-b0bd-46c9-9f14-a72248b01183",
   "metadata": {},
   "source": [
    "**Deepseek R1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea7aca-d396-4a54-af6b-1bc093d97778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_base_url = \"enter-url here.\"\n",
    "#ds_model_name \"enter model name here\"ds_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea24de5-2bd1-4783-b27d-2f230a84c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_base_url = \"enter-url here.\"\n",
    "ds_base_url = \"https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/namespaces/serving-default/endpoints/deepseek-r1-distill-llama-8b/openai/v1\"\n",
    "#ds_model_name \"enter model name here\"\n",
    "ds_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcb22c-5562-4976-b41a-7bb31af5f97a",
   "metadata": {},
   "source": [
    "#### Auth setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78495-5b5a-4ee1-8b33-2d8f1ea766e7",
   "metadata": {},
   "source": [
    "Here is the auth token you'll use to connect to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e226feb-d60f-436d-ae9c-aaf460df8c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJqa3UiOiJodHRwczovL2FpbmYtYXctZGwtbWFzdGVyMC5haW5mLWNkcC52YXliLXhva2cuY2xvdWRlcmEuc2l0ZTo4NDQzL2FpbmYtYXctZGwva3Qta2VyYmVyb3Mva25veHRva2VuL2FwaS92MS9qd2tzLmpzb24iLCJraWQiOiJhbnB3NFN0QkZsTG1tZWN0RU05Z2hOVHBTZ09GdjhuN1RyRExwR3MwVEZBIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJvemFyYXRlIiwiamt1IjoiaHR0cHM6Ly9haW5mLWF3LWRsLW1hc3RlcjAuYWluZi1jZHAudmF5Yi14b2tnLmNsb3VkZXJhLnNpdGU6ODQ0My9haW5mLWF3LWRsL2t0LWtlcmJlcm9zL2tub3h0b2tlbi9hcGkvdjEvandrcy5qc29uIiwia2lkIjoiYW5wdzRTdEJGbExtbWVjdEVNOWdoTlRwU2dPRnY4bjdUckRMcEdzMFRGQSIsImlzcyI6IktOT1hTU08iLCJleHAiOjE3NDAwMjU5NDksIm1hbmFnZWQudG9rZW4iOiJmYWxzZSIsImtub3guaWQiOiJlZGEwMzg1ZC0wYThhLTRmMjgtYTRjMy1iY2Y3YzM1ZjJkNzkifQ.hLK9epFNQgLdCG7BGtYDyB1-rc2JJM7MQ6PGXTrsVRfz4Cibede_vwEktSYhZ_o3wcfzbT8YmO1MxDlaF2Q9ZqF5rmclN1rOiS5KEEJUSHdvLJ8lIy6xBhvR5kbVjeDNQ_C3q0JdGPp6EaeGsjW5-Au8yGIxmH4vywqEg3ps1ilGD7gTOO3Pr2T1vTweG4EVPPIH0_ksVIdH8FSanV2LNSiu4SN_WKNNcQXFqDqI-08OQbS2YbDW7vhnvT_sO2RyS5x66yea-qXrF-jkY-nh4ylVGBKNCw8Jilzn88-HVQTqhpHFpMTXfQAfQfnFyi3rgufvC8-mp-ntA81jqV08fA'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90547c3c-2ec0-433a-bd62-31b7d62862ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce0197-ec1e-4693-82fb-183a84b78a98",
   "metadata": {},
   "source": [
    "## Basic Model Interaction\n",
    "\n",
    "This section demonstrates the simplest way to interact with our deployed model through the OpenAI package. We'll:\n",
    "1. Create a client with our model's endpoint and authentication\n",
    "2. Send a simple message to test the connection\n",
    "3. Display the model's streaming response\n",
    "\n",
    "This represents the most straightforward way to interact with the model, similar to how you might use OpenAI's API. The key difference is that we're using our own deployed model through Cloudera AI's infrastructure.\n",
    "\n",
    "Note: We're using streaming=True in our completion request, which means we'll see the response being generated token by token, providing a more interactive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef92fa88-473e-4bf0-9475-9df2cab9293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "\tbase_url=base_url,\n",
    "\tapi_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9d6836-5df3-4d7b-9f8c-0effe670015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Write a one-sentence definition of GenAI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4d28a9a-9362-4459-af29-f52d826b27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, short for General Artificial Intelligence, refers to a hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence, without being limited to a specific domain or narrow function."
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[{\"role\":\"user\",\"content\":message}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde10c23-aa28-4a30-b241-9a8242db9949",
   "metadata": {},
   "source": [
    "### Using a LangChain API Framework\n",
    "\n",
    "Now we'll demonstrate how to use the same model through LangChain, a popular framework for building LLM applications. This shows how Cloudera AI's models can integrate seamlessly with different frameworks while maintaining the same functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1398b2f7-84de-43d9-afba-bd63f92d4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_chat = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    base_url=base_url,\n",
    "    temperature=0.2,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec27efe-7d5c-4759-9c0b-ca6a33feb260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, short for General Artificial Intelligence, refers to a hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence, without being limited to a specific domain or narrow function."
     ]
    }
   ],
   "source": [
    "# Create the message\n",
    "message = \"Write a one-sentence definition of GenAI.\"\n",
    "messages = [HumanMessage(content=message)]\n",
    "\n",
    "# Stream the response\n",
    "for chunk in lc_chat.stream(messages):\n",
    "    if chunk.content:\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c6688-b736-42f4-8b87-b569b656b78e",
   "metadata": {},
   "source": [
    "## Enhanced Chat Client Implementation\n",
    "\n",
    "This section implements a stateful chat client that maintains conversation history and can handle streaming responses. It demonstrates how to build more complex applications while maintaining the simple interface of the basic client.\n",
    "\n",
    "Key features:\n",
    "- Conversation history tracking\n",
    "- Streaming response support\n",
    "- Configurable parameters\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "aa52d379-0f54-4c23-b6d5-091c1b31116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # Add at the top with other imports\n",
    "import os\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from openai import OpenAI\n",
    "\n",
    "class ChatClient:\n",
    "    def __init__(self, model_name: str, base_url: str, deepseek_clean: bool = False):\n",
    "        self.model_name = model_name\n",
    "        self.deepseek_clean = deepseek_clean\n",
    "        \n",
    "        # Set up HTTP client\n",
    "        if \"CUSTOM_CA_STORE\" not in os.environ:\n",
    "            http_client = httpx.Client()\n",
    "        else:\n",
    "            http_client = httpx.Client(verify=os.environ[\"CUSTOM_CA_STORE\"])\n",
    "            \n",
    "        # Load API key\n",
    "        OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            http_client=http_client,\n",
    "        )\n",
    "        \n",
    "        self.conversation_history: List[Dict[str, str]] = []\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove thinking tags and extract only the actual question/guess.\n",
    "        \"\"\"\n",
    "        # Handle empty or None responses\n",
    "        if not response:\n",
    "            return \"\"\n",
    "            \n",
    "        # First clean up any think blocks and explanatory text\n",
    "        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "        response = re.sub(r'.*\\*\\*Question:\\*\\*\\s*', '', response)\n",
    "        response = re.sub(r'.*Question:\\s*', '', response)\n",
    "        response = re.sub(r'Step-by-Step.*', '', response)\n",
    "        response = re.sub(r'\\*\\*.*?\\*\\*', '', response)\n",
    "        \n",
    "        # Get just the question or guess, taking the last non-empty line\n",
    "        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "        if lines:\n",
    "            actual_response = lines[-1]  # Take last non-empty line\n",
    "            if actual_response.startswith('FINAL GUESS:'):\n",
    "                return actual_response\n",
    "            elif '?' in actual_response:\n",
    "                # Extract just the question\n",
    "                return actual_response.split('?')[0].strip() + '?'\n",
    "                \n",
    "        return response.strip()\n",
    "    def chat(self, message: str, stream: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Send a message to the chat model and get the response.\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                partial_message = \"\"\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=self.conversation_history + [{\"role\": \"system\", \"content\": \"After your thinking, always provide a clear, structured answer.\"}],\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.7,\n",
    "                    max_tokens=1024,  # Further increased token limit\n",
    "                    stream=True,\n",
    "                )\n",
    "                \n",
    "                for chunk in response:\n",
    "                    if chunk.choices[0].delta.content is not None:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        partial_message += content\n",
    "                        if not self.deepseek_clean:\n",
    "                            print(content, end='', flush=True)\n",
    "                \n",
    "                final_message = partial_message\n",
    "                if self.deepseek_clean:\n",
    "                    final_message = self._clean_response(partial_message)\n",
    "                    print(repr(final_message))\n",
    "                    \n",
    "            else:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=self.conversation_history,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.7,\n",
    "                    max_tokens=512,  # Increased token limit\n",
    "                    stream=False,\n",
    "                )\n",
    "                \n",
    "                complete_response = response.choices[0].message.content\n",
    "                #print(\"\\nNon-streaming response:\", repr(complete_response))\n",
    "                \n",
    "                final_message = complete_response\n",
    "                if self.deepseek_clean:\n",
    "                    final_message = self._clean_response(complete_response)\n",
    "                    #print(\"\\nAfter cleaning:\", repr(final_message))\n",
    "                    print(repr(final_message))\n",
    "            # Only add to history if we got a valid response\n",
    "            if final_message:\n",
    "                self.conversation_history.append({\"role\": \"assistant\", \"content\": final_message})\n",
    "            \n",
    "            return final_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in chat method: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the conversation history.\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6603b52-e28c-4ea1-ac41-d41f11a9f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "chat_client = ChatClient(model_name, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8e7891e-27e8-4781-87cf-91a47868cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"in 6 sentences or less explain how weights get update during model training process of a neural network. Explain this to a 6 year old'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6b9e82-520a-4b71-8086-f4d75f7f87e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an explanation of how weights get updated during model training:\n",
      "\n",
      "Imagine you have a robot that can draw a picture, but it's not very good at it. You show the robot lots of pictures and tell it which parts of the picture are correct and which parts are wrong. The robot then tries to make the drawing better by changing the way it uses its crayons (these are like the weights in the neural network). When the robot makes a mistake, it says \"oops, I made a mistake!\" and changes the way it uses its crayons a little bit. It keeps trying and changing its crayons until it gets the picture right. This is kind of like how a neural network updates its weights during training.\n"
     ]
    }
   ],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8d87608-c072-4720-acce-f80407e20e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "message2 = \"now follow it update with learning rate. 5 sentences or less\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "844cd929-ff06-44f0-8ec6-072b62180489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's an explanation of how learning rate affects weight updates:\n",
      "\n",
      "So, when the robot makes a mistake, it changes its crayons a little bit. But, sometimes it might change them too much, and that's not good. That's where the learning rate comes in - it's like a special button that says \"change your crayons by this much\". If the learning rate is high, the robot changes its crayons a lot, but if it's low, the robot changes them just a little bit. This helps the robot learn more smoothly and not make too many mistakes at once.\n"
     ]
    }
   ],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message2, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0375d9-5682-4208-9531-e78227fa1c02",
   "metadata": {},
   "source": [
    "## Model Switching Demonstration\n",
    "\n",
    "One of the key benefits of Cloudera AI is the ability to easily switch between different models. Here we'll demonstrate this by changing to the Deepseek model while using the same code structure.\n",
    "\n",
    "For this section, we'll use our second model's information:\n",
    "- URL goes into `ds_base_url` (remember to clip after /v1)\n",
    "- Model ID goes into `ds_model_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "894716a3-b63a-4188-bd9b-f252b95827cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a116c314-bdb2-47e5-bbda-a863a35130a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "deep_seek_chat_client = ChatClient(ds_model_name,ds_base_url,deepseek_clean = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e4b43b-d8dc-4012-ae6c-e1c52baa30b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "caa6dbf9-f958-42e0-bfb3-2b7349fe6e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The learning rate is a crucial hyperparameter in neural networks that determines how quickly and effectively the model learns from data during training. It is integral to optimization algorithms like gradient descent, influencing the size of weight adjustments. A high learning rate can lead to instability and overshooting, while a low rate slows training but may improve accuracy. Balancing the learning rate is essential for stable convergence, often adjusted during training through techniques like learning rate scheduling. Ultimately, it's a key factor in achieving an optimal model.\"\n"
     ]
    }
   ],
   "source": [
    "deep_seek_chat_client = ChatClient(ds_model_name, ds_base_url, deepseek_clean=True)\n",
    "message3 = \"in 5 sentences or less what is a learning rate in neural networks?\"\n",
    "response_ds = deep_seek_chat_client.chat(message3, stream=True)\n",
    "#print(response_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8c8dab43-1e9e-480a-92a4-37c76a1d5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwentyQuestionsGame:\n",
    "    def __init__(\n",
    "        self,\n",
    "        answerer_client: ChatClient,  # Use existing ChatClient for answerer\n",
    "        guesser_client: ChatClient,   # Use existing ChatClient for guesser\n",
    "        max_questions=10,\n",
    "        delay_seconds=2\n",
    "    ):\n",
    "        \"\"\"Initialize game with two ChatClient instances\"\"\"\n",
    "        self.answerer_client = answerer_client\n",
    "        self.guesser_client = guesser_client\n",
    "        self.secret_item = None\n",
    "        self.questions_asked = 0\n",
    "        self.max_questions = max_questions\n",
    "        self.delay_seconds = delay_seconds\n",
    "        self.game_history = []\n",
    "\n",
    "    def format_history(self):\n",
    "        if not self.game_history:\n",
    "            return \"None\"\n",
    "        return \"\\n\".join([f\"Q{h['question_number']}: {h['question']} -> {h['answer']}\" \n",
    "                     for h in self.game_history])\n",
    "    \n",
    "    def play_game(self):\n",
    "        try:\n",
    "            # Clear any existing conversation history\n",
    "            self.answerer_client.clear_history()\n",
    "            self.guesser_client.clear_history()\n",
    "            \n",
    "            # Get the secret item from the answerer\n",
    "            answerer_prompt = \"\"\"\n",
    "            You are playing a game of 20 questions. You need to think of an item (it can be an object, \n",
    "            person, place, or concept) and keep it secret. Only share the item in your response, \n",
    "            nothing else. The other AI will try to guess it through yes/no questions.\n",
    "            \"\"\"\n",
    "            \n",
    "            self.secret_item = self.answerer_client.chat(answerer_prompt, stream=False)\n",
    "            print(f\"The item to guess is: {self.secret_item.strip()}\")\n",
    "            \n",
    "            while self.questions_asked < self.max_questions:\n",
    "                guesser_prompt = f\"\"\"\n",
    "You are playing 20 questions. Questions asked: {self.questions_asked}\n",
    "Previous questions and answers:\n",
    "{self.format_history()}\n",
    "Questions remaining: {self.max_questions - self.questions_asked}\n",
    "\n",
    "RULES:\n",
    "1. Ask only ONE yes/no question\n",
    "2. Never repeat a previous question\n",
    "3. If you know it's a transportation tool, ask about specific types (car, bike, train, etc.)\n",
    "4. When confident, make your guess with 'FINAL GUESS: [item]'\n",
    "5. No explanations - just the question\n",
    "\n",
    "Remember: This is 20 questions - use each question to narrow down possibilities!\n",
    "\"\"\"\n",
    "                \n",
    "                raw_response = self.guesser_client.chat(guesser_prompt, stream=False)\n",
    "                \n",
    "                # Extract just the actual question, removing any explanatory text\n",
    "                question = raw_response.split('**Question:**')[-1].strip() if '**Question:**' in raw_response else raw_response.strip()\n",
    "                \n",
    "                if \"FINAL GUESS:\" in question:\n",
    "                    final_guess = question.split(\"FINAL GUESS:\")[1].strip()\n",
    "                    print(f\"\\nFinal guess made: {final_guess}\")\n",
    "                    print(f\"The actual item was: {self.secret_item}\")\n",
    "                    print(f\"Game ended after {self.questions_asked} questions\")\n",
    "                    return\n",
    "                else:\n",
    "                    self.questions_asked += 1\n",
    "                    time.sleep(self.delay_seconds)\n",
    "                    \n",
    "                    answerer_prompt = f\"\"\"\n",
    "                    You are playing a game of 20 questions. The item you chose is: {self.secret_item}\n",
    "                    The question asked is: {question}\n",
    "                    Please answer only with 'Yes' or 'No'.\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    answer = self.answerer_client.chat(answerer_prompt, stream=False)\n",
    "                    \n",
    "                    # Store clean interaction\n",
    "                    self.game_history.append({\n",
    "                        'question': question,\n",
    "                        'answer': answer.strip(),\n",
    "                        'question_number': self.questions_asked\n",
    "                    })\n",
    "                    \n",
    "                    # Display clean interaction\n",
    "                    print(f\"\\nQuestion {self.questions_asked}: {question}\")\n",
    "                    print(f\"Answer: {answer.strip()}\")\n",
    "                \n",
    "                time.sleep(self.delay_seconds)\n",
    "                \n",
    "            print(f\"\\nGame Over! Maximum questions ({self.max_questions}) reached.\")\n",
    "            print(f\"The item was: {self.secret_item}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the game: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bacc5afc-7dfd-47e7-aae1-ed6de72cefa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The item to guess is: A car\n",
      "'Is it a car?'\n",
      "\n",
      "Question 1: Is it a car?\n",
      "Answer: Yes\n",
      "'Is it an SUV?'\n",
      "\n",
      "Question 2: Is it an SUV?\n",
      "Answer: Yes\n",
      "'Is it a Honda CR-V?'\n",
      "\n",
      "Question 3: Is it a Honda CR-V?\n",
      "Answer: Yes\n",
      "'FINAL GUESS: Honda CR-V'\n",
      "\n",
      "Final guess made: Honda CR-V\n",
      "The actual item was: A car\n",
      "Game ended after 3 questions\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def main():\n",
    "    # Create ChatClient instances\n",
    "    answerer_client = ChatClient(\n",
    "        model_name=model_name,\n",
    "        base_url=base_url\n",
    "    )\n",
    "    guesser_client = ChatClient(\n",
    "        model_name=ds_model_name,\n",
    "        base_url= ds_base_url, deepseek_clean=True)\n",
    "    # Create and run the game\n",
    "    game = TwentyQuestionsGame(\n",
    "        answerer_client=answerer_client,\n",
    "        guesser_client=guesser_client,\n",
    "        max_questions=15,\n",
    "        delay_seconds=1\n",
    "    )\n",
    "    game.play_game()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd91b88-f969-4fd1-bb00-b743721dc008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
