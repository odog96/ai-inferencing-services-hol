{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d257d154-5507-4f61-ab1f-a05e87eaf435",
   "metadata": {},
   "source": [
    "# Model API Integration with Cloudera AI\n",
    "\n",
    "This notebook demonstrates the flexibility of Cloudera AI inferencing services by showing different ways to interact with deployed models. We'll progress from basic API usage to more complex implementations, showing how easy it is to switch between different models and frameworks.\n",
    "\n",
    "## Requirements\n",
    "- Python 3.10 or later\n",
    "- Access to Cloudera AI console\n",
    "- Two deployed models: test-model-llama-8b-v2 and deepseek-r1-distill-llama-8b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d30e1a-fad2-4a45-971f-329d133a4f19",
   "metadata": {},
   "source": [
    "## Model Setup\n",
    "\n",
    "Before proceeding, you'll need to gather information from your deployed models in the Cloudera AI console:\n",
    "\n",
    "1. Go to Cloudera AI console > Model Endpoints\n",
    "2. Find the models: \n",
    "   - test-model-llama-8b-v2\n",
    "   - deepseek-r1-distill-llama-8b\n",
    "3. For each model:\n",
    "   - Copy the endpoint URL (remove everything after /v1) for example :\n",
    "   - `https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/...../modelxyz/openai/v1/chat/completions`\n",
    "   - would be converted to :\n",
    "   - `https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/...../modelxyz/openai/v1`\n",
    "   - Copy the Model ID\n",
    "\n",
    "The first model's information will go into `base_url` and `model_name` variables. The 2nd model will be `ds_base_url` and `ds_model_name` variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715f17e3-fef9-461e-b4a2-d0af5631b93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "# For Lang chain:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import asyncio\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa72c8-1ff2-4146-8382-2e04d913ec11",
   "metadata": {},
   "source": [
    "#### Model enpoint collection\n",
    "1. Collect Llama 3.1 model endpont details\n",
    "2. Collect Deepseek model enpoint details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29991406-dd0e-4da8-8c74-03ad956cc830",
   "metadata": {},
   "source": [
    "go to cloudera AI and get the following parameters.Cut off tail end of url after '/v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447e6f82-538a-4f32-ac2d-a2f38d7ba169",
   "metadata": {},
   "source": [
    "**Llama 3.1 8b**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af687a-1b4e-4be9-929f-dbd74ba23e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"enter-url here.\"\n",
    "#model_name = \"enter model name here\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665ddd00-b0bd-46c9-9f14-a72248b01183",
   "metadata": {},
   "source": [
    "**Deepseek R1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ea7aca-d396-4a54-af6b-1bc093d97778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_base_url = \"enter-url here.\"\n",
    "#ds_model_name \"enter model name here\"ds_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbcb22c-5562-4976-b41a-7bb31af5f97a",
   "metadata": {},
   "source": [
    "#### Auth setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78495-5b5a-4ee1-8b33-2d8f1ea766e7",
   "metadata": {},
   "source": [
    "Here is the auth token you'll use to connect to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e226feb-d60f-436d-ae9c-aaf460df8c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90547c3c-2ec0-433a-bd62-31b7d62862ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef92fa88-473e-4bf0-9475-9df2cab9293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "\tbase_url=base_url,\n",
    "\tapi_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce0197-ec1e-4693-82fb-183a84b78a98",
   "metadata": {},
   "source": [
    "## Basic Model Interaction\n",
    "\n",
    "This section demonstrates the simplest way to interact with our deployed model through the OpenAI package. We'll:\n",
    "1. Create a client with our model's endpoint and authentication\n",
    "2. Send a simple message to test the connection\n",
    "3. Display the model's streaming response\n",
    "\n",
    "This represents the most straightforward way to interact with the model, similar to how you might use OpenAI's API. The key difference is that we're using our own deployed model through Cloudera AI's infrastructure.\n",
    "\n",
    "Note: We're using streaming=True in our completion request, which means we'll see the response being generated token by token, providing a more interactive experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d6836-5df3-4d7b-9f8c-0effe670015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Write a one-sentence definition of GenAI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d28a9a-9362-4459-af29-f52d826b27e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[{\"role\":\"user\",\"content\":message}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde10c23-aa28-4a30-b241-9a8242db9949",
   "metadata": {},
   "source": [
    "### Using a LangChain API Framework\n",
    "\n",
    "Now we'll demonstrate how to use the same model through LangChain, a popular framework for building LLM applications. This shows how Cloudera AI's models can integrate seamlessly with different frameworks while maintaining the same functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1398b2f7-84de-43d9-afba-bd63f92d4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_chat = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    base_url=base_url,\n",
    "    temperature=0.2,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec27efe-7d5c-4759-9c0b-ca6a33feb260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the message\n",
    "message = \"Write a one-sentence definition of GenAI.\"\n",
    "messages = [HumanMessage(content=message)]\n",
    "\n",
    "# Stream the response\n",
    "for chunk in lc_chat.stream(messages):\n",
    "    if chunk.content:\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c6688-b736-42f4-8b87-b569b656b78e",
   "metadata": {},
   "source": [
    "## Enhanced Chat Client Implementation\n",
    "\n",
    "This section implements a stateful chat client that maintains conversation history and can handle streaming responses. It demonstrates how to build more complex applications while maintaining the simple interface of the basic client.\n",
    "\n",
    "Key features:\n",
    "- Conversation history tracking\n",
    "- Streaming response support\n",
    "- Configurable parameters\n",
    "- Error handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa52d379-0f54-4c23-b6d5-091c1b31116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatClient:\n",
    "    def __init__(self, model_name: str, base_url: str, deepseek_clean: bool = False):\n",
    "        self.model_name = model_name\n",
    "        self.deepseek_clean = deepseek_clean\n",
    "        \n",
    "        # Set up HTTP client\n",
    "        if \"CUSTOM_CA_STORE\" not in os.environ:\n",
    "            http_client = httpx.Client()\n",
    "        else:\n",
    "            http_client = httpx.Client(verify=os.environ[\"CUSTOM_CA_STORE\"])\n",
    "            \n",
    "        # Load API key\n",
    "        OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            http_client=http_client,\n",
    "        )\n",
    "        \n",
    "        self.conversation_history: List[Dict[str, str]] = []\n",
    "\n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove thinking tags and extract only the actual question/guess.\n",
    "        \"\"\"\n",
    "        # Handle empty or None responses\n",
    "        if not response:\n",
    "            return \"\"\n",
    "            \n",
    "        # First clean up any think blocks and explanatory text\n",
    "        response = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "        response = re.sub(r'.*\\*\\*Question:\\*\\*\\s*', '', response)\n",
    "        response = re.sub(r'.*Question:\\s*', '', response)\n",
    "        response = re.sub(r'Step-by-Step.*', '', response)\n",
    "        response = re.sub(r'\\*\\*.*?\\*\\*', '', response)\n",
    "        \n",
    "        # Get just the question or guess, taking the last non-empty line\n",
    "        lines = [line.strip() for line in response.split('\\n') if line.strip()]\n",
    "        if lines:\n",
    "            actual_response = lines[-1]  # Take last non-empty line\n",
    "            if actual_response.startswith('FINAL GUESS:'):\n",
    "                return actual_response\n",
    "            elif '?' in actual_response:\n",
    "                # Extract just the question\n",
    "                return actual_response.split('?')[0].strip() + '?'\n",
    "                \n",
    "        return response.strip()\n",
    "    def chat(self, message: str, stream: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Send a message to the chat model and get the response.\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        try:\n",
    "            if stream:\n",
    "                partial_message = \"\"\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=self.conversation_history + [{\"role\": \"system\", \"content\": \"After your thinking, always provide a clear, structured answer.\"}],\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.7,\n",
    "                    max_tokens=1024,  # Further increased token limit\n",
    "                    stream=True,\n",
    "                )\n",
    "                \n",
    "                for chunk in response:\n",
    "                    if chunk.choices[0].delta.content is not None:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        partial_message += content\n",
    "                        if not self.deepseek_clean:\n",
    "                            print(content, end='', flush=True)\n",
    "                \n",
    "                final_message = partial_message\n",
    "                if self.deepseek_clean:\n",
    "                    final_message = self._clean_response(partial_message)\n",
    "                    print(repr(final_message))\n",
    "                    \n",
    "            else:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model_name,\n",
    "                    messages=self.conversation_history,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.7,\n",
    "                    max_tokens=512,  # Increased token limit\n",
    "                    stream=False,\n",
    "                )\n",
    "                \n",
    "                complete_response = response.choices[0].message.content\n",
    "                #print(\"\\nNon-streaming response:\", repr(complete_response))\n",
    "                \n",
    "                final_message = complete_response\n",
    "                if self.deepseek_clean:\n",
    "                    final_message = self._clean_response(complete_response)\n",
    "                    #print(\"\\nAfter cleaning:\", repr(final_message))\n",
    "                    print(repr(final_message))\n",
    "            # Only add to history if we got a valid response\n",
    "            if final_message:\n",
    "                self.conversation_history.append({\"role\": \"assistant\", \"content\": final_message})\n",
    "            \n",
    "            return final_message\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in chat method: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the conversation history.\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6603b52-e28c-4ea1-ac41-d41f11a9f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "chat_client = ChatClient(model_name, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e7891e-27e8-4781-87cf-91a47868cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"in 6 sentences or less explain how weights get update during model training process of a neural network. Explain this to a 6 year old'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b9e82-520a-4b71-8086-f4d75f7f87e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d87608-c072-4720-acce-f80407e20e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "message2 = \"now follow it update with learning rate. 5 sentences or less\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844cd929-ff06-44f0-8ec6-072b62180489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message2, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0375d9-5682-4208-9531-e78227fa1c02",
   "metadata": {},
   "source": [
    "## Model Switching Demonstration\n",
    "\n",
    "One of the key benefits of Cloudera AI is the ability to easily switch between different models. Here we'll demonstrate this by changing to the Deepseek model while using the same code structure.\n",
    "\n",
    "For this section, we'll use our second model's information:\n",
    "- URL goes into `ds_base_url` (remember to clip after /v1)\n",
    "- Model ID goes into `ds_model_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40acccd7-c5bb-4eb3-a5d7-0bfa6667c62f",
   "metadata": {},
   "source": [
    "**Primer on Deepseek**\n",
    "there is a ton of information out there about deepseek, how it was trained at a fraction of the cost of traditional massive scale LLMs. But today we're going to narrow the scope to usage. You'll notice that deepseek r1 'thinks' as it response. This chain of thought allows user to see how the model breaks down the problem into sub steps to arrive at an answer. \n",
    "\n",
    "For this lab, we've configured a class that allows deepseek to respond in its natural way, but also provide a way to supress that and give you only the desired response, with deepseek_clean = False or True, parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a116c314-bdb2-47e5-bbda-a863a35130a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "deep_seek_chat_client = ChatClient(ds_model_name,ds_base_url,deepseek_clean = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aed5cd-e100-4fed-a563-8210894eb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message3 = \"in 5 sentences or less what is a learning rate in neural networks?\"\n",
    "response_ds = deep_seek_chat_client.chat(message3, stream=True)\n",
    "#print(response_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa6dbf9-f958-42e0-bfb3-2b7349fe6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_seek_chat_client = ChatClient(ds_model_name, ds_base_url, deepseek_clean=True)\n",
    "message3 = \"in 5 sentences or less what is a learning rate in neural networks?\"\n",
    "response_ds = deep_seek_chat_client.chat(message3, stream=True)\n",
    "#print(response_ds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
