{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Working With Pinecone Vector Database\n",
    "Although Vector Databases have existed long before Large Language Models, vector DBs have become an important part of many LLM solutions. In particular, Retreival Augmented Generation (or RAG) architecture addresses LLM's halucinations and issues with longer-term memory by augmenting the user's prompt with the results of a search accross a vector DB. [Pinecone](https://www.pinecone.io/) is a cloud-based vector database that is easy to integrate with your CML workflow, as this notebook shows. \n",
    "\n",
    "Recall that in the previous exervice you created CML jobs to scrape a site and load each page into Pinecone vector DB. This notebook will focus on interacting with Pinecone from a Jupyter notebook.\n",
    "\n",
    "![Exercise 3 overview](../assets/exercise_3.png)\n",
    "\n",
    "### 3.1 Imports and global vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cdsw/.local/lib/python3.10/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pinecone\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "EMBEDDING_MODEL_REPO = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "PINECONE_REGION = os.getenv('PINECONE_REGION')\n",
    "\n",
    "PINECONE_INDEX = os.getenv('PINECONE_INDEX')\n",
    "dimension = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Pinecone connection\n",
    "Pinecone client is initialized with the parameters defined previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialising Pinecone connection...\n",
      "Pinecone initialised\n",
      "Getting 'cml-index-se-west' as object...\n",
      "Success\n",
      "Total number of embeddings in Pinecone index is 15.\n"
     ]
    }
   ],
   "source": [
    "print(\"initialising Pinecone connection...\")\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(\"Pinecone initialised\")\n",
    "\n",
    "print(f\"Getting '{PINECONE_INDEX}' as object...\")\n",
    "index = pc.Index(PINECONE_INDEX)\n",
    "print(\"Success\")\n",
    "\n",
    "# Get latest statistics from index\n",
    "current_collection_stats = index.describe_index_stats()\n",
    "print('Total number of embeddings in Pinecone index is {}.'.format(current_collection_stats.get('total_vector_count')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Function to peform the vector search \n",
    "The idea is to find a chunk from the Knowledge Base that is \"close\" to what the original user's prompt is. We perform a semantic search using the user's question, find the nearest knowledge base chunk, and return the content of that chunk along with its source and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/namespaces/serving-default/endpoints/mistral-4b-rerank-a10g/v1\"\n",
    "model_name = \"nvidia/nv-rerankqa-mistral-4b-v3\"\n",
    "# Load API key\n",
    "OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get embeddings for a user question and query Pinecone vector DB for nearest knowledge base chunk\n",
    "def get_nearest_chunk_from_pinecone_vectordb(index, question):\n",
    "    # Generate embedding for user question with embedding model\n",
    "    retriever = SentenceTransformer(EMBEDDING_MODEL_REPO)\n",
    "    xq = retriever.encode([question]).tolist()\n",
    "    xc = index.query(vector=xq, top_k=5,\n",
    "                 include_metadata=True)\n",
    "    \n",
    "    matching_files = []\n",
    "    scores = []\n",
    "    for i, match,  in enumerate(xc['matches']):\n",
    "        # extract the 'file_path' within 'metadata'\n",
    "        file_path = match['metadata']['file_path']\n",
    "        print('file_path',file_path)\n",
    "        print('chunch rank',i)\n",
    "        # extract the individual scores for each vector\n",
    "        score = match['score']\n",
    "        scores.append(score)\n",
    "        matching_files.append(file_path)\n",
    "\n",
    "    # Return text of the nearest knowledge base chunk \n",
    "    # Note that this ONLY uses the first matching document for semantic search. matching_files holds the top results so you can increase this if desired.\n",
    "    response = load_context_chunk_from_data(matching_files[0])\n",
    "    sources = matching_files[0]\n",
    "    score = scores[0]\n",
    "    return response, sources, score\n",
    "\n",
    "# Return the Knowledge Base doc based on Knowledge Base ID (relative file path)\n",
    "def load_context_chunk_from_data(id_path):\n",
    "    with open(id_path, \"r\") as f: # Open file in read mode\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Examine the results of the vector search\n",
    "Given the text of the question, we can now perform a vector search and output the results in the notebook. An important detail here is the ability to interact with metadata (e.g. context source) which can be used to narrow down the search space and, more critically, for authorization. These approaches are out of scope of this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/product/topics/ml-product-overview.txt\n",
      "chunch rank 0\n",
      "file_path /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-critical-and-noncritical-pods.txt\n",
      "chunch rank 1\n",
      "file_path /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-runtimes.txt\n",
      "chunch rank 2\n",
      "file_path /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview1.txt\n",
      "chunch rank 3\n",
      "file_path /home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/architecture-overview/topics/ml-architecture-overview-provisioning.txt\n",
      "chunch rank 4\n",
      "\n",
      "Context Chunk: \n",
      "What is CML?\n",
      "Cloudera AI overviewCloudera Docs\n",
      "Cloudera AI overview\n",
      "Machine learning has become one of the most critical capabilities for modern businesses\n",
      " to grow and stay competitive today. From automating internal processes to optimizing the design,\n",
      " creation, and marketing processes behind virtually every product consumed, machine learning models have\n",
      " permeated almost every aspect of our work and personal lives.\n",
      "Machine learning development is iterative and complex, made even harder because most machine learning tools are not built\n",
      "for the entire machine learning lifecycle. Cloudera AI accelerates time-to-value by enabling data scientists to collaborate in\n",
      "a single unified platform that is all inclusive for powering any AI use case. Purpose-built\n",
      "for agile experimentation and production machine learning workflows, Cloudera AI manages everything from data preparation to MLOps,\n",
      "to predictive reporting. You can solve mission critical machine learning challenges along the entire lifecycle with\n",
      "greater speed and agility to discover opportunities which can mean the difference for your\n",
      "business.\n",
      "Each Cloudera AI Workbench enables teams of data scientists to develop, test, train, and ultimately\n",
      "deploy machine learning models for building predictive applications all on the data under\n",
      "management within the enterprise data cloud. Cloudera AI Workbench support fully-containerized\n",
      "execution of Python, R, Scala, and Spark workloads through flexible and extensible engines. \n",
      "\n",
      "\n",
      "AI applications\n",
      "Analytical Applications provide a place to host long running applications within a Cloudera AI\n",
      " project.\n",
      "While Cloudera AI offers a place for Data Scientists to perform advanced analytics and models into\n",
      "production, Analytical Applications provides a place to host long running applications within\n",
      "a Cloudera AI project. This opens up a larger group of users to the insights discovered in Cloudera AI.\n",
      "Applications can be built with a variety of frameworks like Flask and Streamlit. They run\n",
      "within their own isolated compute instance which keeps them from timing out and they take\n",
      "advantage of ML Runtimes. Applications are accessible to users through the web. Applications\n",
      "can be for a variety of use cases like hosting interactive data visualizations or providing a\n",
      "UI frontend for a deployed mode in Cloudera AI. \n",
      "\n",
      "\n",
      "Exploratory Data Science\n",
      "Cloudera AI enables data practitioners to discover, query, and easily visualize their data sets\n",
      " all within a single user interface.\n",
      "The beginning of every data science project begins with finding and understanding the data\n",
      "you need. Cloudera AI brings all the tools you need for exploratory data analysis together in a single\n",
      "UI so that data practitioners don't have to jump between applications, and IT doesn't have to\n",
      "support multiple tools. Cloudera AI provides users with a view of available data assets that they can\n",
      "connect to, a sql editor to query those data sources, and an easy-to-use drag-and-drop\n",
      "visualization tool to understand your data and communicate insights.\n",
      "\n",
      "\n",
      "ML Ops\n",
      "Cloudera AI enables users to deploy machine learning and other models into\n",
      " production.\n",
      "Cloudera AI enables users to deploy machine learning and other models into production, either as a\n",
      "batch process through the Jobs functionality, or as near-real-time REST APIs using the Models\n",
      "functionality. In addition, Cloudera AI provides a number of features to help maintain, monitor and\n",
      "govern these models in production. The Model Governance feature ensures that every deployed\n",
      "Model is tracked in the Cloudera Data Catalog, and allows the user to specify which data\n",
      "tables were used to train the model in order to provide model-data lineage. Deployed Models\n",
      "have a built-in dashboard for monitoring technical metrics relating to deployed Cloudera AI Models,\n",
      "such as request throughput, latency, and resource consumption. Additionally, users can track\n",
      "arbitrary business metrics relating to each inference event, and match the results with\n",
      "delayed metrics from a data warehouse or other source using an automatically generated UUID.\n",
      "By analyzing these metrics, the user can assess the model for aggregated metrics such as\n",
      "accuracy on an ongoing basis.\n",
      "\n",
      "\n",
      "Core capabilities\n",
      "This section details the core capabilities for Cloudera AI.\n",
      "Cloudera AI covers the end-to-end machine learning workflow,\n",
      "enabling fully isolated and containerized workloads - including Python, R, and\n",
      "Spark-on-Kubernetes - for scale-out data engineering and machine learning with seamless\n",
      "distributed dependency management. \n",
      "\n",
      "\n",
      "Sessions enable Data Scientists to directly leverage the CPU, memory,\n",
      " and GPU compute available across the Cloudera AI Workbenches, while also being directly connected to the\n",
      " data in the data lake.\n",
      "\n",
      "\n",
      "Experiments enable Data Scientists to run multiple variations of model\n",
      " training workloads, tracking the results of each Experiment in order to train the best\n",
      " possible Model.\n",
      "\n",
      "\n",
      "Models can be deployed in a matter of clicks, removing any roadblocks to\n",
      " production. They are served as REST endpoints in a high availability manner, with\n",
      " automated lineage building and metric tracking for MLOps purposes.\n",
      "\n",
      "\n",
      "Jobs can be used to orchestrate an entire end-to-end automated pipeline,\n",
      " including monitoring for model drift and automatically kicking off model re-training and\n",
      " re-deployment as needed. \n",
      "\n",
      "\n",
      "Applications deliver interactive experiences for business users in a\n",
      " matter of clicks. Frameworks such as Flask and Shiny can be used in development of these\n",
      " Applications, while Cloudera Data Visualization is also available as a point-and-click\n",
      " interface for building these experiences.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cloudera AI benefits\n",
      "This section details the Cloudera AI benefits for each type of\n",
      " user.\n",
      "Cloudera AI is built for the agility and power of cloud\n",
      "computing, but is not limited to any one provider or data source. It is a comprehensive\n",
      "platform to collaboratively build and deploy machine learning capabilities at scale. \n",
      "Cloudera AI provides benefits for each type of user.\n",
      "Data Scientists\n",
      "\n",
      "\n",
      "Enable DS teams to collaborate and speed model development and delivery with\n",
      " transparent, secure, and governed workflows\n",
      "\n",
      "\n",
      "Expand AI use cases with automated Cloudera AI pipelines and an integrated and complete\n",
      " production Cloudera AI toolkit \n",
      "\n",
      "\n",
      "Empower faster decision making and trust with end-to-end visibility and\n",
      " auditability of data, processes, models, and dashboards\n",
      "\n",
      "\n",
      "IT\n",
      "\n",
      "\n",
      "Increase data service productivity with visibility, security, and governance of the\n",
      " complete machine learning lifecycle \n",
      "\n",
      "\n",
      "Eliminate silos, blindspots, and the need to move/duplicate data with a fully\n",
      " integrated platform across the data lifecycle. \n",
      "\n",
      "\n",
      "Accelerate AI with self-service access and containerized Cloudera AI Workbench that\n",
      " remove the heavy lifting and get models to production faster \n",
      "\n",
      "\n",
      "Business Users\n",
      "\n",
      "\n",
      "Access interactive Applications built and deployed by DS teams.\n",
      "\n",
      "\n",
      "Be empowered with predictive insights to more intelligently make business\n",
      " decisions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Key differences between Cloudera AI (Cloudera Public Cloud) and Cloudera Data Science\n",
      " Workbench\n",
      "This topic highlights some key differences between Cloudera Data Science Workbench and\n",
      " its cloud-native counterpart, Cloudera AI. \n",
      "How is Cloudera AI related to Cloudera Data Science\n",
      "  Workbench (CDSW)?\n",
      "Cloudera AI expands the end-to-end workflow of Cloudera Data Science\n",
      "Workbench (CDSW) with cloud-native benefits like rapid provisioning,\n",
      "elastic autoscaling, distributed dependency isolation, and distributed GPU\n",
      "training.\n",
      "It can run its own native distributed computing workloads without requiring a\n",
      "separate CDH cluster for scale-out compute. It is designed to run on Cloudera Data Platform in existing\n",
      "Kubernetes environments, reducing operational costs for some customers while delivering\n",
      "multi-cloud portability. On Cloudera Public Cloud, managed cloud Kubernetes services include EKS, AKS,\n",
      "or GKE, and on Cloudera Private Cloud, they include Red Hat OpenShift Container Platform or Cloudera Embedded Container Service.\n",
      "Both products help data engineers and data science teams be more\n",
      "productive on shared data and compute, with strong security and\n",
      "governance. They share extensive code.\n",
      "There is one primary difference:\n",
      "\n",
      "CDSW extends an existing CDH cluster, by running on\n",
      "gateway nodes and pushing distributed compute workloads to the\n",
      "cluster. CDSW requires and supports a single CDH cluster for its\n",
      "distributed compute, including Apache Spark.\n",
      "\n",
      "\n",
      "In contrast, Cloudera AI is self-contained and manages its own\n",
      "distributed compute, natively running workloads - including but not\n",
      "limited to Apache Spark - in containers on Kubernetes.\n",
      "Note: It can still connect to an existing cluster to\n",
      "leverage its distributed compute, data, or metadata (SDX).\n",
      "\n",
      "\n",
      "Table 1. Key Differences\n",
      "\n",
      "\n",
      "CDSW\n",
      "Cloudera AI\n",
      "\n",
      "\n",
      "\n",
      "Architecture\n",
      "\n",
      "CDSW can run on a  Cloudera Data Platform-DC, CDH (5 or 6), and HDP cluster and runs on\n",
      "one or more dedicated gateway nodes on the cluster. \n",
      "\n",
      "\n",
      "Cloudera AI is self-contained and does not require an\n",
      "attached CDH/HDP cluster. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Notion of 1 master and multiple worker hosts. \n",
      "No designated master and worker hosts; all nodes\n",
      " are ephemeral. \n",
      "\n",
      "\n",
      "Security\n",
      "Kerberos authentication integrated via the\n",
      " CDH/HDP cluster\n",
      "Centralised identity management\n",
      " using FreeIPA via the Cloudera Data Platform.\n",
      "\n",
      "\n",
      "\n",
      "External authentication via LDAP/SAML.\n",
      "\n",
      "\n",
      "App Storage\n",
      "Project files, internal postgresDB, and Livelog,\n",
      " are all stored persistently on the Master host. \n",
      "All required persistent storage is on\n",
      " cloud-managed block store, NFS, and a  relational data store.\n",
      " For example, for AWS, this is managed via EFS. \n",
      "\n",
      "\n",
      "Compute\n",
      "Python/R/Scala workloads run on the CDSW\n",
      " gateway nodes of the cluster. \n",
      "Python/R/Scala workloads run on the\n",
      " Cloudera Data Platform/cloud-provider-managed K8s cluster. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CDSW pushes distributed compute workloads, such as\n",
      "Spark-on-YARN, to the CDH/HDP cluster. \n",
      "\n",
      "Spark-on-YARN is not supported; Spark-on-K8s\n",
      " instead. Workloads will run on a dedicated K8s cluster\n",
      " provisioned within the customer environment. \n",
      "\n",
      "\n",
      "\n",
      "No autoscaling. \n",
      "Autoscaling via your cloud service provider.\n",
      " Kubernetes/node-level autoscaling will be used to\n",
      " expand/contract the cluster size based on demand. \n",
      "\n",
      "\n",
      "Packaging\n",
      "Available as a downloadable RPM and CSD. \n",
      "Available as a managed service on Cloudera Data Platform.\n",
      "\n",
      "\n",
      "\n",
      "Spark is packaged with CDH.\n",
      "Spark on K8s is packaged with Cloudera AI - no dependency\n",
      " on an external cluster.\n",
      "\n",
      "\n",
      "Data Access\n",
      "Data usually resides on the attached CDH/HDP\n",
      " cluster in HDFS, Hive, HBase, and so on. \n",
      "Data can reside on object storage such as S3 or\n",
      " any pre-existing workload clusters registered with Cloudera Data Platform. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Context Source(s): \n",
      "/home/cdsw/data/https:/docs.cloudera.com/machine-learning/cloud/product/topics/ml-product-overview.txt\n",
      "\n",
      "Pinecone Score: \n",
      "0.421064794\n"
     ]
    }
   ],
   "source": [
    "question = \"What is CML?\" ## (Swap with your own based on your dataset)3\n",
    "context_chunk, sources, score = get_nearest_chunk_from_pinecone_vectordb(index, question)\n",
    "print(\"\\nContext Chunk: \")\n",
    "print(context_chunk)\n",
    "print(\"\\nContext Source(s): \")\n",
    "print(sources)\n",
    "print(\"\\nPinecone Score: \")\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = load_context_chunk_from_data(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ML RuntimesCloudera Docs ML Runtimes ML Runtimes are responsible for running data science workloads and intermediating       access to the underlying cluster.  Cloudera AI allows you to run any code via an interactive session, scheduled job, or deployed model          or application. Data Scientists can use interactive sessions to explore data, or develop a          model. They can create jobs and schedule them to run at specified times or productionize          their work as a model to provide a REST endpoint or as an application that offers an          interactive data dashboard for business users. All of these workloads run inside an ML Runtimes container on top of Kubernetes.  Cloudera ML Runtimes are purpose built to serve a specific use-case. They are available          with a single editor (for example, Workbench, Jupyterlab), ship a single language Kernel          (for example, Python 3.8 or R 4.0), and have a set of UNIX tools and utilities or language          libraries and packages.  ML Runtimes have been open sourced and are available in the cloudera/ml-runtimes GitHub repository. If you need to understand your Runtime          environments fully or want to build a new Runtime from scratch,  you can access the          Dockerfiles that were used to build the ML Runtimes container images in this repository. There is a wide range of supported Runtimes out-of-the-box that cover the large majority of          Data Science use-cases, but any special requirements can be satisfied by building a custom          ML Runtimes container image. Cloudera AI also supports quota management for CPU, GPU, and memory to limit the amount of          resources users have access to within the Cloudera AI Workbench.  Before ML Runtimes, Cloudera AI offered similar functionalities via Legacy Engines. These          deprecated container images followed a monolithic architecture, all kernels (Python, R,          Scala), and all seemingly useful packages and libraries were included in the image.   Parent topic: Architecture Overview'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Takeaways\n",
    "* Vector search is a critical component of any LLM app using RAG architecture\n",
    "* Cloudera's partner [Pinecone](https://www.pinecone.io/) provides a convenient SaaS offering for a Vector Database to support LLM RAG architecture\n",
    "* Metadata from each entry in the Vector DB can be used to refine searches and add custom authorization frameworks\n",
    "\n",
    "### Up Next: Go to Exercise 4\n",
    "Where a gradio app is launched to complete the first iteration of the Q&A chat use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
