{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "295cacb5-b676-4925-8677-e2a177047fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# must run on => Python 3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715f17e3-fef9-461e-b4a2-d0af5631b93e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhttpx\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import httpx\n",
    "import json\n",
    "from typing import List, Dict, Generator\n",
    "# For Lang chain:\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import asyncio\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d30e1a-fad2-4a45-971f-329d133a4f19",
   "metadata": {},
   "source": [
    "**Enter url below**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29991406-dd0e-4da8-8c74-03ad956cc830",
   "metadata": {},
   "source": [
    "go to cloudera AI and get the following parameters.Cut off tail end of url after '/v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caa6acc2-9ae3-4b0f-b4d6-7b19d8d6659d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base_url = \"enter-url here.\"\n",
    "base_url = \"https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/namespaces/serving-default/endpoints/test-model-llama-8b-v2/v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6ababe-6268-433f-9656-3c30e551cf38",
   "metadata": {},
   "source": [
    "**Enter Model name here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aaedd8e-52bf-4bea-8f62-208ee3e65a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"enter model name here\"\n",
    "model_name = \"meta/llama-3.1-8b-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b78495-5b5a-4ee1-8b33-2d8f1ea766e7",
   "metadata": {},
   "source": [
    "Here is the auth token you'll use to connect to your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e226feb-d60f-436d-ae9c-aaf460df8c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJqa3UiOiJodHRwczovL2FpbmYtYXctZGwtbWFzdGVyMC5haW5mLWNkcC52YXliLXhva2cuY2xvdWRlcmEuc2l0ZTo4NDQzL2FpbmYtYXctZGwva3Qta2VyYmVyb3Mva25veHRva2VuL2FwaS92MS9qd2tzLmpzb24iLCJraWQiOiJhbnB3NFN0QkZsTG1tZWN0RU05Z2hOVHBTZ09GdjhuN1RyRExwR3MwVEZBIiwiYWxnIjoiUlMyNTYifQ.eyJzdWIiOiJvemFyYXRlIiwiamt1IjoiaHR0cHM6Ly9haW5mLWF3LWRsLW1hc3RlcjAuYWluZi1jZHAudmF5Yi14b2tnLmNsb3VkZXJhLnNpdGU6ODQ0My9haW5mLWF3LWRsL2t0LWtlcmJlcm9zL2tub3h0b2tlbi9hcGkvdjEvandrcy5qc29uIiwia2lkIjoiYW5wdzRTdEJGbExtbWVjdEVNOWdoTlRwU2dPRnY4bjdUckRMcEdzMFRGQSIsImlzcyI6IktOT1hTU08iLCJleHAiOjE3NDAwMDA1OTQsIm1hbmFnZWQudG9rZW4iOiJmYWxzZSIsImtub3guaWQiOiI1NGZkODk5Zi01YWViLTQ0YjQtYWU2Ni01NjRkZjUxN2QzMmMifQ.W6sZTLb9R7vrxHfYNqwzBDLUA1YMdP3N0ILDohvfsUhJrDqLF9WDQ4Pgy94_nTkBq6ZcGH-p0uTgmPIOgKhiHOe1BD8nZwWYscASAUKvEvgG3FYY98XzY64GinNg6wkTGpJt_9NOXVnIVzBLEzhTYgZhp-NRK8b6toX6iIugyCwAFMbTG2BSsSaNmP3QAaf7BzrLczfuhc8waR1003QIxQaOM8ABG_iOoW_oWzXHp7Dqa8HOSSGaFswXVWnYjw6Wp1NmdraJOCkDlh0A1dbE-17DVNMibO4SM4So-j4Hnu6vhwRp4pM0Iz6OYyooVLDtPx_6SCTMjN1yXqcIYhCNDQ'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90547c3c-2ec0-433a-bd62-31b7d62862ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ce0197-ec1e-4693-82fb-183a84b78a98",
   "metadata": {},
   "source": [
    "Basic Connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef92fa88-473e-4bf0-9475-9df2cab9293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "\tbase_url=base_url,\n",
    "\tapi_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9d6836-5df3-4d7b-9f8c-0effe670015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Write a one-sentence definition of GenAI.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4d28a9a-9362-4459-af29-f52d826b27e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, short for General Artificial Intelligence, refers to a hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence, without being limited to a specific domain or narrow function."
     ]
    }
   ],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=model_name,\n",
    "  messages=[{\"role\":\"user\",\"content\":message}],\n",
    "  temperature=0.2,\n",
    "  top_p=0.7,\n",
    "  max_tokens=1024,\n",
    "  stream=True\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde10c23-aa28-4a30-b241-9a8242db9949",
   "metadata": {},
   "source": [
    "### Using a Lang Chain API Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1398b2f7-84de-43d9-afba-bd63f92d4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_chat = ChatOpenAI(\n",
    "    model_name=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    base_url=base_url,\n",
    "    temperature=0.2,\n",
    "    streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ec27efe-7d5c-4759-9c0b-ca6a33feb260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenAI, short for General Artificial Intelligence, refers to a hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence, without being limited to a specific domain or narrow function."
     ]
    }
   ],
   "source": [
    "# Create the message\n",
    "message = \"Write a one-sentence definition of GenAI.\"\n",
    "messages = [HumanMessage(content=message)]\n",
    "\n",
    "# Stream the response\n",
    "for chunk in lc_chat.stream(messages):\n",
    "    if chunk.content:\n",
    "        print(chunk.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455c6688-b736-42f4-8b87-b569b656b78e",
   "metadata": {},
   "source": [
    "### Chat use-case\n",
    "Now we'll build a class that will allow us to carry a conversation with our model.\n",
    "Lets shift back to openai api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa52d379-0f54-4c23-b6d5-091c1b31116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatClient:\n",
    "    def __init__(self, model_name: str, base_url: str, deepseek_clean: bool = False):\n",
    "        self.model_name = model_name\n",
    "        self.deepseek_clean = deepseek_clean\n",
    "        \n",
    "        # Set up HTTP client\n",
    "        if \"CUSTOM_CA_STORE\" not in os.environ:\n",
    "            http_client = httpx.Client()\n",
    "        else:\n",
    "            http_client = httpx.Client(verify=os.environ[\"CUSTOM_CA_STORE\"])\n",
    "            \n",
    "        # Load API key\n",
    "        OPENAI_API_KEY = json.load(open(\"/tmp/jwt\"))[\"access_token\"]\n",
    "        \n",
    "        # Initialize OpenAI client\n",
    "        self.client = OpenAI(\n",
    "            base_url=base_url,\n",
    "            api_key=OPENAI_API_KEY,\n",
    "            http_client=http_client,\n",
    "        )\n",
    "        \n",
    "        self.conversation_history: List[Dict[str, str]] = []\n",
    "    \n",
    "    def _clean_response(self, response: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove thinking tags from Deepseek model responses.\n",
    "        \n",
    "        Args:\n",
    "            response: The raw response string from the model\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned response with thinking sections removed\n",
    "        \"\"\"\n",
    "        import re\n",
    "        # Remove everything between <think> and </think> tags\n",
    "        cleaned = re.sub(r'<think>.*?</think>', '', response, flags=re.DOTALL)\n",
    "        # Remove any extra whitespace that might be left\n",
    "        cleaned = re.sub(r'\\n\\s*\\n', '\\n', cleaned)\n",
    "        return cleaned.strip()\n",
    "    \n",
    "    def chat(self, message: str, stream: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Send a message to the chat model and get the response.\n",
    "        \n",
    "        Args:\n",
    "            message: The message to send to the model\n",
    "            stream: Whether to stream the response or return it all at once\n",
    "            \n",
    "        Returns:\n",
    "            The complete response as a string\n",
    "        \"\"\"\n",
    "        # Add user message to history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": message})\n",
    "        \n",
    "        if stream:\n",
    "            partial_message = \"\"\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=self.conversation_history,\n",
    "                temperature=1,\n",
    "                top_p=0.7,\n",
    "                max_tokens=256,\n",
    "                stream=True,\n",
    "            )\n",
    "            \n",
    "            for chunk in response:\n",
    "                if chunk.choices[0].delta.content is not None:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    partial_message += content\n",
    "                    print(content, end='', flush=True)\n",
    "            \n",
    "            print()  # New line after response is complete\n",
    "            \n",
    "            # Clean the response if deepseek_clean is enabled\n",
    "            if self.deepseek_clean:\n",
    "                partial_message = self._clean_response(partial_message)\n",
    "                \n",
    "            # Add complete response to history\n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": partial_message})\n",
    "            return partial_message\n",
    "            \n",
    "        else:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=self.conversation_history,\n",
    "                stream=False,\n",
    "            )\n",
    "            complete_response = response.choices[0].message.content\n",
    "            \n",
    "            # Clean the response if deepseek_clean is enabled\n",
    "            if self.deepseek_clean:\n",
    "                complete_response = self._clean_response(complete_response)\n",
    "                \n",
    "            self.conversation_history.append({\"role\": \"assistant\", \"content\": complete_response})\n",
    "            return complete_response\n",
    "    \n",
    "    def get_history(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get the conversation history.\"\"\"\n",
    "        return self.conversation_history\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d6603b52-e28c-4ea1-ac41-d41f11a9f55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "chat_client = ChatClient(model_name, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8e7891e-27e8-4781-87cf-91a47868cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"\"\"in 6 sentences or less explain how weights get update during model training process of a neural network. Explain this to a 6 year old?'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e6b9e82-520a-4b71-8086-f4d75f7f87e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chat_client' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For streaming responses (will print as it receives chunks):\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_client\u001b[49m\u001b[38;5;241m.\u001b[39mchat(message, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'chat_client' is not defined"
     ]
    }
   ],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "response = chat_client.chat(message, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8d87608-c072-4720-acce-f80407e20e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "message2 = \"can you make connection to this meditatino and the Turing test?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "844cd929-ff06-44f0-8ec6-072b62180489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "#response = chat_client.chat(message2, stream=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0375d9-5682-4208-9531-e78227fa1c02",
   "metadata": {},
   "source": [
    "### Deepseek\n",
    "Okay Now let's switch gears. Lets see how simple it is to change models with the same code, Don't forget to clip the end of the url.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "894716a3-b63a-4188-bd9b-f252b95827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_base_url = \"enter-url here.\"\n",
    "ds_base_url = \"https://ai-inference.ainf-cdp.vayb-xokg.cloudera.site/namespaces/serving-default/endpoints/deepseek-r1-distill-llama-8b/openai/v1\"\n",
    "#ds_model_name \"enter model name here\"\n",
    "ds_model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a116c314-bdb2-47e5-bbda-a863a35130a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the chat client\n",
    "deep_seek_chat_client = ChatClient(ds_model_name,ds_base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e1e07528-8559-46a3-b1ee-159147e54eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#message3 = \"what do you suspect happened before the big bang. Was there time dilation or did time exist?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9563304-d38f-45b8-9477-4b89cd9f0d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For streaming responses (will print as it receives chunks):\n",
    "#response_ds = deep_seek_chat_client.chat(message3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ae86498-c10d-4d3e-ba57-9c496de661e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#message4 = \"create a sudoku puzzle that is challenging\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a29a2325-523a-4a5d-a857-b81384d305c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#response_ds = deep_seek_chat_client.chat(message4, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c8dab43-1e9e-480a-92a4-37c76a1d5df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwentyQuestionsGame:\n",
    "    def __init__(\n",
    "        self,\n",
    "        answerer_client: ChatClient,  # Use existing ChatClient for answerer\n",
    "        guesser_client: ChatClient,   # Use existing ChatClient for guesser\n",
    "        max_questions=10,\n",
    "        delay_seconds=2\n",
    "    ):\n",
    "        \"\"\"Initialize game with two ChatClient instances\"\"\"\n",
    "        self.answerer_client = answerer_client\n",
    "        self.guesser_client = guesser_client\n",
    "        self.secret_item = None\n",
    "        self.questions_asked = 0\n",
    "        self.max_questions = max_questions\n",
    "        self.delay_seconds = delay_seconds\n",
    "        self.game_history = []\n",
    "\n",
    "    def play_game(self):\n",
    "        try:\n",
    "            # Clear any existing conversation history\n",
    "            self.answerer_client.clear_history()\n",
    "            self.guesser_client.clear_history()\n",
    "            \n",
    "            # Get the secret item from the answerer\n",
    "            answerer_prompt = \"\"\"\n",
    "            You are playing a game of 20 questions. You need to think of an item (it can be an object, \n",
    "            person, place, or concept) and keep it secret. Only share the item in your response, \n",
    "            nothing else. The other AI will try to guess it through yes/no questions.\n",
    "            \"\"\"\n",
    "            \n",
    "            self.secret_item = self.answerer_client.chat(answerer_prompt, stream=False)\n",
    "            print(f\"[SECRET - Only for human] The item to guess is: {self.secret_item}\")\n",
    "            \n",
    "            while self.questions_asked < self.max_questions:\n",
    "                guesser_prompt = f\"\"\"\n",
    "                You are playing a game of 20 questions. You have asked {self.questions_asked} questions so far.\n",
    "                Game history: {self.game_history}\n",
    "                You have {self.max_questions - self.questions_asked} questions remaining.\n",
    "                If you know what the item is, make your guess by stating 'FINAL GUESS: [your guess]'.\n",
    "                Otherwise, ask your next yes/no question.\n",
    "                \"\"\"\n",
    "                \n",
    "                response = self.guesser_client.chat(guesser_prompt, stream=False)\n",
    "                \n",
    "                if \"FINAL GUESS:\" in response:\n",
    "                    final_guess = response.split(\"FINAL GUESS:\")[1].strip()\n",
    "                    print(f\"\\nFinal guess made: {final_guess}\")\n",
    "                    print(f\"The actual item was: {self.secret_item}\")\n",
    "                    print(f\"Game ended after {self.questions_asked} questions\")\n",
    "                    return\n",
    "                else:\n",
    "                    # Treat response as a question\n",
    "                    question = response\n",
    "                    self.questions_asked += 1\n",
    "                    \n",
    "                    # Add delay between transactions\n",
    "                    time.sleep(self.delay_seconds)\n",
    "                    \n",
    "                    # Get answer from answerer\n",
    "                    answerer_prompt = f\"\"\"\n",
    "                    You are playing a game of 20 questions. The item you chose is: {self.secret_item}\n",
    "                    The question asked is: {question}\n",
    "                    Please answer only with 'Yes' or 'No'.\n",
    "                    \"\"\"\n",
    "                    \n",
    "                    answer = self.answerer_client.chat(answerer_prompt, stream=False)\n",
    "                    \n",
    "                    # Record the interaction\n",
    "                    self.game_history.append({\n",
    "                        'question': question.strip(),\n",
    "                        'answer': answer.strip(),\n",
    "                        'question_number': self.questions_asked\n",
    "                    })\n",
    "                    \n",
    "                    # Display the interaction\n",
    "                    print(f\"\\nQuestion {self.questions_asked}: {question.strip()}\")\n",
    "                    print(f\"Answer: {answer.strip()}\")\n",
    "                \n",
    "                # Add delay between rounds\n",
    "                time.sleep(self.delay_seconds)\n",
    "            \n",
    "            # If we reach here, we've run out of questions\n",
    "            print(f\"\\nGame Over! Maximum questions ({self.max_questions}) reached.\")\n",
    "            print(f\"The item was: {self.secret_item}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the game: {str(e)}\")\n",
    "            raise\n",
    "# class TwentyQuestionsGame:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         answerer_client: ChatClient,  # Use existing ChatClient for answerer\n",
    "#         guesser_client: ChatClient,   # Use existing ChatClient for guesser\n",
    "#         max_questions=10,\n",
    "#         delay_seconds=2\n",
    "#     ):\n",
    "#         \"\"\"Initialize game with two ChatClient instances\"\"\"\n",
    "#         self.answerer_client = answerer_client\n",
    "#         self.guesser_client = guesser_client\n",
    "#         self.secret_item = None\n",
    "#         self.questions_asked = 0\n",
    "#         self.max_questions = max_questions\n",
    "#         self.delay_seconds = delay_seconds\n",
    "#         self.game_history = []\n",
    "\n",
    "#     def play_game(self):\n",
    "#         try:\n",
    "#             # Clear any existing conversation history\n",
    "#             self.answerer_client.clear_history()\n",
    "#             self.guesser_client.clear_history()\n",
    "            \n",
    "#             # Get the secret item from the answerer\n",
    "#             answerer_prompt = \"\"\"\n",
    "#             You are playing a game of 20 questions. You need to think of an item (it can be an object, \n",
    "#             person, place, or concept) and keep it secret. Only share the item in your response, \n",
    "#             nothing else. The other AI will try to guess it through yes/no questions.\n",
    "#             \"\"\"\n",
    "            \n",
    "#             self.secret_item = self.answerer_client.chat(answerer_prompt, stream=False)\n",
    "#             print(f\"[SECRET - Only for human] The item to guess is: {self.secret_item}\")\n",
    "            \n",
    "#             while self.questions_asked < self.max_questions:\n",
    "#                 # Get question from guesser\n",
    "#                 guesser_prompt = f\"\"\"\n",
    "#                 You are playing a game of 20 questions. You need to guess what the item is by asking\n",
    "#                 yes/no questions. You have asked {self.questions_asked} questions so far.\n",
    "#                 Game history: {self.game_history}\n",
    "#                 Please ask your next yes/no question to try to identify the item.\n",
    "#                 Only provide the question, nothing else.\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 question = self.guesser_client.chat(guesser_prompt, stream=False)\n",
    "#                 self.questions_asked += 1\n",
    "                \n",
    "#                 # Add delay between transactions\n",
    "#                 time.sleep(self.delay_seconds)\n",
    "                \n",
    "#                 # Get answer from answerer\n",
    "#                 answerer_prompt = f\"\"\"\n",
    "#                 You are playing a game of 20 questions. The item you chose is: {self.secret_item}\n",
    "#                 The question asked is: {question}\n",
    "#                 Please answer only with 'Yes' or 'No'.\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 answer = self.answerer_client.chat(answerer_prompt, stream=False)\n",
    "                \n",
    "#                 # Record the interaction\n",
    "#                 self.game_history.append({\n",
    "#                     'question': question.strip(),\n",
    "#                     'answer': answer.strip(),\n",
    "#                     'question_number': self.questions_asked\n",
    "#                 })\n",
    "                \n",
    "#                 # Display the interaction\n",
    "#                 print(f\"\\nQuestion {self.questions_asked}: {question.strip()}\")\n",
    "#                 print(f\"Answer: {answer.strip()}\")\n",
    "                \n",
    "#                 # Add delay between transactions\n",
    "#                 time.sleep(self.delay_seconds)\n",
    "                \n",
    "#                 # After each question, let the guesser make a final guess\n",
    "#                 guess_prompt = f\"\"\"\n",
    "#                 Based on the following game history, would you like to make a final guess at what the item is?\n",
    "#                 If yes, state 'FINAL GUESS: [your guess]'. If no, just respond with 'CONTINUE'.\n",
    "#                 Game history: {self.game_history}\n",
    "#                 \"\"\"\n",
    "                \n",
    "#                 guess_response = self.guesser_client.chat(guess_prompt, stream=False)\n",
    "                \n",
    "#                 if \"FINAL GUESS:\" in guess_response:\n",
    "#                     final_guess = guess_response.split(\"FINAL GUESS:\")[1].strip()\n",
    "#                     print(f\"\\nFinal guess made: {final_guess}\")\n",
    "#                     print(f\"The actual item was: {self.secret_item}\")\n",
    "#                     print(f\"Game ended after {self.questions_asked} questions\")\n",
    "#                     return\n",
    "            \n",
    "#             # If we reach here, we've run out of questions\n",
    "#             print(f\"\\nGame Over! Maximum questions ({self.max_questions}) reached.\")\n",
    "#             print(f\"The item was: {self.secret_item}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"An error occurred during the game: {str(e)}\")\n",
    "#             raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bacc5afc-7dfd-47e7-aae1-ed6de72cefa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SECRET - Only for human] The item to guess is: Book\n",
      "\n",
      "Question 1: <think>\n",
      "Okay, so I'm trying to figure out what the item is that the random selection is referring to. I know that each question narrows down possible options by 50%, so I should aim to ask a question that splits the remaining possibilities roughly in half each time.\n",
      "\n",
      "But wait, I just asked the first question about whether it's alive and got a \"yes.\" So now I know that the item is alive. My next step is to ask another question that narrows down the category further. Let me think about common categories of alive things. Maybe starting with something like whether it's an animal. That makes sense because animals are a broad category that can lead to more specific questions. \n",
      "\n",
      "Alternatively, I could think about whether it's a plant, but I'm not sure if that's as broad. Maybe instead, I can think about whether it's something like a dog or a cat. But that might not be the best approach because the number of possible animals could be too varied. I should stick with a general category to keep the tree wide so that I have enough options to ask questions later on.\n",
      "\n",
      "Wait, but I just realized that the previous question was about being alive, and the answer was yes. So the item is alive, like a living thing. So the next question should be about the type of living thing. Maybe I can ask if it has a backbone because that divides into animals, humans, and other invertebrates. But perhaps that's a bit specific. Alternatively, I can ask if it's a vertebrate or not, which would include fish, reptiles, birds, etc. But maybe a better approach is to ask about whether it's a mammal, which would include even more specific options within animals, like dogs, cats, etc. Wait, but I'm not sure if that's the best way because there are other types of animals beyond mammals.\n",
      "\n",
      "Alternatively, maybe I can ask if it's an animal, which is a more general category that would include mammals, birds, fish, reptiles, etc., but that might not be as helpful because it's too broad. Hmm, perhaps I should think about the most straightforward way to split the possibilities in half.\n",
      "\n",
      "Wait, the previous answer was yes, it's alive. So now I know it's a living thing. So my next question should be about the category of that living thing. Maybe I can ask if it's an animal to keep it simple. So, \"Is the item an animal?\" That way, if yes, then I can proceed to ask within that category, and if no, maybe it's a plant or something else. But the user hasn't indicated if it's a plant or not, so maybe starting with \"Is it an animal?\" is better because it's a common category that will split the remaining possibilities in half, allowing more focused questions next time.\n",
      "</think>\n",
      "\n",
      "The next strategic question to ask is: \"Is the item an animal?\" This question effectively divides the remaining possibilities into two halves, allowing for further narrowing down in subsequent questions. \n",
      "\n",
      "Question: Is the item an animal?\n",
      "Answer: No\n",
      "\n",
      "Question 2: <think>\n",
      "Okay, so I've asked the first question, which was, \"Is the item an animal?\" and the answer was \"No.\" Hmm, that's interesting. I thought it might be an animal, but apparently not. So, since it's not an animal, I need to think about what other categories fit. The next step is to figure out what kind of living thing it could be. Since it's not an animal, maybe it's something that's alive but not classified as an animal, like a plant.\n",
      "\n",
      "But wait, is there something else besides plants or maybe a microorganism? Hmm, perhaps I should ask a question that narrows it down more. Maybe I can ask if it's a plant. That seems logical because plants are a large category too. So, \"Is the item a plant?\" That could help me understand if it's a plant or something else, like a fungus or maybe a bacterium. But I'm not sure if that's the best approach because there are different types of plants, and that might not be the most efficient way to narrow it down.\n",
      "\n",
      "Alternatively, I could ask a more general question about whether it's a living thing that isn't an animal. Wait, but I already know it's alive, so maybe I should focus on other characteristics. If it's not an animal, perhaps it's a fungus, or maybe it's a microorganism. How can I figure that out?\n",
      "\n",
      "Maybe it's better to ask if it's a fungus. Fungi are a big category too, but I'm not sure if that's the best path. Or perhaps I can ask if it's a bacterium. But I'm concerned that asking about specific categories might be too narrow and might not give me enough options to proceed further.\n",
      "\n",
      "Wait, maybe I should think about how many questions I have left. I have 14 more questions, so I don't want to limit my options too early. If I can ask a broad question that splits the possibilities into two halves, that might be better. But I'm not sure how else to categorize it because I already know it's alive and not an animal. Maybe it's time to ask about the type of cell it has or something like that. But the user hasn't mentioned anything about cells, so that might not be applicable.\n",
      "\n",
      "Alternatively, could it be a virus? Viruses are non-living entities, but are they considered alive? Wait, viruses can be alive, but they're not cells. They can reproduce and cause disease. But I think the previous 'no' answer was about being alive, so maybe it's a virus? But I'm not certain. \n",
      "\n",
      "This is getting a bit confusing. I need to make sure I'm asking the right question to keep the options open and not limit myself unnecessarily. Maybe sticking with broader categories is still the way to go. So, \"Is the item a plant?\" seems safer because it's a common category that could lead to further specific questions about types of plants, but if the answer is negative, I can consider other categories like fungi or bacteria.\n",
      "\n",
      "Yeah, I think I'll go with that. It's a logical next step that keeps the game going without boxing myself in too early.\n",
      "</think>\n",
      "\n",
      "The next strategic question to ask is: \"Is the item a plant?\" This question effectively narrows down the possibilities further and allows for potential follow-up questions related to plant species or characteristics.\n",
      "Answer: Yes\n",
      "\n",
      "Question 3: The next strategic question to ask is: \"Is the item a plant?\" This question narrows down the possibilities further and opens the door for more specific questions about the type of plant, if the answer is affirmative.\n",
      "Answer: No\n",
      "\n",
      "Question 4: The next strategic question to ask is: \"Is the item a plant?\" This question narrows down the possibilities further and opens the door for more specific questions about the type of plant, if the answer is affirmative.\n",
      "Answer: I already answered this question earlier. The correct answer is \"Yes\" (because the original item \"Book\" was 'Yes' when answering \"Is the item a plant?\", not 'No').\n",
      "\n",
      "Question 5: The next strategic question to ask is: \"Is the item a fungus?\" This question narrows down the possibilities further and opens the door for more specific questions about the type of fungus, if the answer is affirmative.\n",
      "Answer: No\n",
      "\n",
      "Question 6: The next strategic question to ask is: \"Is the item a fungus?\" This question narrows down the possibilities further and opens the door for more specific questions about the type of fungus, if the answer is affirmative.\n",
      "Answer: No, I already answered this question earlier. The next question I'll consider is: \"Is the item typically held in your hand?\"\n",
      "\n",
      "Question 7: The next strategic question to ask is: \"Is the item typically held in your hand?\" This question is more specific and can help narrow down the type of object further based on its common usage.\n",
      "Answer: Yes\n",
      "\n",
      "Question 8: The next strategic question to ask is: \"Is the item typically held in your hand?\" This question is more specific and can help narrow down the type of object further based on its common usage.\n",
      "Answer: No, I already answered this question earlier. The original answer is 'Yes' because it's assumed that, typically a book is held and held may sometimes be with one's hands, but not necessarily just in one hand such as with a fruit or a pen.\n",
      "\n",
      "Question 9: The next strategic question to ask is: \"Is the item typically held in your hand?\" This question is more specific and can help narrow down the type of object further based on its common usage.\n",
      "Answer: Yes\n",
      "\n",
      "Question 10: The next strategic question to ask is: \"Is the item typically held in your hand?\" This question is more specific and can help narrow down the type of object further based on its common usage.\n",
      "\n",
      "---\n",
      "\n",
      "Alright, let's break this down step by step. Starting from the beginning, the goal is to narrow down the item quickly and effectively using yes/no questions. The process begins with determining whether the item is alive or not. Once it's established that the item is alive, the next step is to determine its category, such as whether it's an animal, plant, fungus, or something else. \n",
      "\n",
      "1. **First Question:** Is the item alive?  \n",
      "   - If yes, proceed with further categorization.  \n",
      "   - If no, consider other categories like plants or non-living entities.\n",
      "\n",
      "2. **Second Question:** If the item is alive, ask if it's an animal.  \n",
      "   - If yes, delve deeper into animal specifics like mammals, birds, etc.  \n",
      "   - If no, explore other alive but non-animal categories, like plants.\n",
      "\n",
      "3. **Third Question:** If not an animal, ask if it's a plant.  \n",
      "   - This helps identify if we're dealing with flora or another category.\n",
      "\n",
      "4. **Fourth Question:** If not a plant, ask if it's a fungus.  \n",
      "   - Fungi are another broad category that can lead to more specific questions.\n",
      "\n",
      "5. **Subsequent Questions:** Depending on previous answers, ask more specific questions to zero in on the exact item. For example, asking if the item is typically held in your hand can help determine its common usage, which can then be tied back to known categories.\n",
      "\n",
      "By following this strategic approach, each question ideally halves the possibilities, allowing for efficient narrowing down of the item in question.\n",
      "Answer: Yes\n",
      "\n",
      "Final guess made: [your guess]' or ask your next yes/no question if you don't have enough information to guess.\n",
      "The actual item was: Book\n",
      "Game ended after 10 questions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Create ChatClient instances\n",
    "    answerer_client = ChatClient(\n",
    "        model_name=model_name,\n",
    "        base_url=base_url\n",
    "    )\n",
    "    \n",
    "    guesser_client = ChatClient(\n",
    "        model_name=ds_model_name,\n",
    "        base_url= ds_base_url\n",
    "    )\n",
    "    \n",
    "    # Create and run the game\n",
    "    game = TwentyQuestionsGame(\n",
    "        answerer_client=answerer_client,\n",
    "        guesser_client=guesser_client,\n",
    "        max_questions=15,\n",
    "        delay_seconds=2\n",
    "    )\n",
    "    game.play_game()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd91b88-f969-4fd1-bb00-b743721dc008",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
